## 运行环境与加速库注意事项（vLLM）

### 当前结论
- 我们的推理后端使用 vLLM（backend 固定为 `vllm-async-engine`）。
- FlashInfer 是 vLLM 的采样加速库（加速 top‑k/top‑p 等），与 sglang 无关。
- flash-attn 是注意力算子加速库；在 vision 路径不稳定时 vLLM 会回退到 xformers。

### CUDA 版本与 FlashInfer 安装
- 实测环境：`CUDA Version: 12.0`（Driver 525.89.02）。
- 官方预编译轮子仅提供：cu118 / cu121 / cu122 / cu123 / cu124；没有 cu120。
- 因此在 CUDA 12.0 上执行 `pip install flashinfer` 会报：`No matching distribution found`。

#### 解决方案（推荐）
1) 升级驱动/运行时到 CUDA ≥ 12.1（例如驱动 535/550 系列），然后按对应版本安装：
```bash
pip install -U pip setuptools wheel
# 按你的 CUDA 版本二选一（示例）：
pip install -U flashinfer-cu121  # CUDA 12.1
# 或
pip install -U flashinfer-cu122  # CUDA 12.2
pip install -U flashinfer-cu123  # CUDA 12.3
pip install -U flashinfer-cu124  # CUDA 12.4
```
安装完成后重启服务，日志中将不再出现 “FlashInfer is not available.”

2) 不升级驱动：可以尝试源码编译 FlashInfer，但对 CUDA 12.0 支持不稳定，不推荐。
3) 保持现状：继续使用 vLLM 的 PyTorch 采样回退，性能略慢但稳定可用。

### 关于 flash-attn（可选）
- 可能带来注意力算子性能提升，但需严格匹配 CUDA/驱动/显卡/编译环境：
```bash
pip install flash-attn --no-build-isolation
```
- 若 vision 路径有已知问题，vLLM 会自动回退到 xformers（安全但较慢）。

### 诊断与验证
- 查看 CUDA 版本：
```bash
nvidia-smi
```
- 看到如下信息表示当前为 CUDA 12.0：
```
NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0
```
- 服务启动日志中若出现：
  - `FlashInfer is not available. Falling back to the PyTorch-native implementation` ⇒ 未启用 FlashInfer，加速未生效；
  - `Using Flash Attention backend` 或明确的 flash‑attn 成功加载信息 ⇒ 注意力算子加速可能已启用；
  - `vllm cache_config_info`、`EngineCore ... vLLM` ⇒ 表示正在使用 vLLM。

### 总结
- 若追求更高吞吐与更低延迟，优先升级到 CUDA ≥ 12.1 并安装对应 `flashinfer-cuXXX`。
- flash-attn 视环境与稳定性需求选择性启用；遇到问题可回退 xformers。

