"""
MinerU Tianshu - API Server
å¤©æ¢APIæœåŠ¡å™¨

æä¾›RESTful APIæ¥å£ç”¨äºä»»åŠ¡æäº¤ã€æŸ¥è¯¢å’Œç®¡ç†
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import tempfile
from pathlib import Path
from loguru import logger
import uvicorn
from typing import Optional
from datetime import datetime
import os
import re
import uuid
import json
from minio import Minio

from task_db import TaskDB

from mineru.backend.pipeline.pipeline_middle_json_mkcontent import union_make as pipeline_union_make
from mineru.backend.vlm.vlm_middle_json_mkcontent import union_make as vlm_union_make


# åˆå§‹åŒ– FastAPI åº”ç”¨
app = FastAPI(
    title="MinerU Tianshu API",
    description="å¤©æ¢ - ä¼ä¸šçº§å¤šGPUæ–‡æ¡£è§£ææœåŠ¡",
    version="1.0.0"
)

# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ•°æ®åº“
db = TaskDB()

# é…ç½®è¾“å‡ºç›®å½•
OUTPUT_DIR = Path('/tmp/mineru_tianshu_output')
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# MinIO é…ç½®
MINIO_CONFIG = {
    'endpoint': os.getenv('MINIO_ENDPOINT', ''),
    'access_key': os.getenv('MINIO_ACCESS_KEY', ''),
    'secret_key': os.getenv('MINIO_SECRET_KEY', ''),
    'secure': True,
    'bucket_name': os.getenv('MINIO_BUCKET', '')
}


def get_minio_client():
    """è·å–MinIOå®¢æˆ·ç«¯å®ä¾‹"""
    return Minio(
        endpoint=MINIO_CONFIG['endpoint'],
        access_key=MINIO_CONFIG['access_key'],
        secret_key=MINIO_CONFIG['secret_key'],
        secure=MINIO_CONFIG['secure']
    )


def process_markdown_images(md_content: str, image_dir: Path, upload_images: bool = False):
    """
    å¤„ç† Markdown ä¸­çš„å›¾ç‰‡å¼•ç”¨
    
    Args:
        md_content: Markdown å†…å®¹
        image_dir: å›¾ç‰‡æ‰€åœ¨ç›®å½•
        upload_images: æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ° MinIO å¹¶æ›¿æ¢é“¾æ¥
        
    Returns:
        å¤„ç†åçš„ Markdown å†…å®¹
    """
    if not upload_images:
        return md_content
    
    try:
        minio_client = get_minio_client()
        bucket_name = MINIO_CONFIG['bucket_name']
        minio_endpoint = MINIO_CONFIG['endpoint']
        
        # æŸ¥æ‰¾æ‰€æœ‰ markdown æ ¼å¼çš„å›¾ç‰‡
        img_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        
        def replace_image(match):
            alt_text = match.group(1)
            image_path = match.group(2)
            
            # æ„å»ºå®Œæ•´çš„æœ¬åœ°å›¾ç‰‡è·¯å¾„
            full_image_path = image_dir / Path(image_path).name
            
            if full_image_path.exists():
                # è·å–æ–‡ä»¶åç¼€
                file_extension = full_image_path.suffix
                # ç”Ÿæˆ UUID ä½œä¸ºæ–°æ–‡ä»¶å
                new_filename = f"{uuid.uuid4()}{file_extension}"
                
                try:
                    # ä¸Šä¼ åˆ° MinIO
                    object_name = f"images/{new_filename}"
                    minio_client.fput_object(bucket_name=bucket_name, object_name=object_name, file_path=str(full_image_path))
                    
                    # ç”Ÿæˆ MinIO è®¿é—® URL
                    scheme = 'https' if MINIO_CONFIG['secure'] else 'http'
                    minio_url = f"{scheme}://{minio_endpoint}/{bucket_name}/{object_name}"
                    
                    # è¿”å› HTML æ ¼å¼çš„ img æ ‡ç­¾
                    return f'<img src="{minio_url}" alt="{alt_text}">'
                except Exception as e:
                    logger.error(f"Failed to upload image to MinIO: {e}")
                    return match.group(0)  # ä¸Šä¼ å¤±è´¥ï¼Œä¿æŒåŸæ ·
            
            return match.group(0)
        
        # æ›¿æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        new_content = re.sub(img_pattern, replace_image, md_content)
        return new_content
        
    except Exception as e:
        logger.error(f"Error processing markdown images: {e}")
        return md_content  # å‡ºé”™æ—¶è¿”å›åŸå†…å®¹


def read_json_file(file_path: Path):
    """
    è¯»å– JSON æ–‡ä»¶

    Args:
        file_path: JSON æ–‡ä»¶è·¯å¾„

    Returns:
        è§£æåçš„ JSON æ•°æ®ï¼Œå¤±è´¥è¿”å› None
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Failed to read JSON file {file_path}: {e}")
        return None


def get_file_metadata(file_path: Path):
    """
    è·å–æ–‡ä»¶å…ƒæ•°æ®

    Args:
        file_path: æ–‡ä»¶è·¯å¾„

    Returns:
        åŒ…å«æ–‡ä»¶å…ƒæ•°æ®çš„å­—å…¸
    """
    if not file_path.exists():
        return None

    stat = file_path.stat()
    return {
        'size': stat.st_size,
        'created_at': datetime.fromtimestamp(stat.st_ctime).isoformat(),
        'modified_at': datetime.fromtimestamp(stat.st_mtime).isoformat()
    }


def get_images_info(image_dir: Path, upload_to_minio: bool = False):
    """
    è·å–å›¾ç‰‡ç›®å½•ä¿¡æ¯

    Args:
        image_dir: å›¾ç‰‡ç›®å½•è·¯å¾„
        upload_to_minio: æ˜¯å¦ä¸Šä¼ åˆ° MinIO

    Returns:
        å›¾ç‰‡ä¿¡æ¯å­—å…¸
    """
    if not image_dir.exists() or not image_dir.is_dir():
        return {
            'count': 0,
            'list': [],
            'uploaded_to_minio': False
        }

    # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
    image_extensions = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp', '.svg'}
    image_files = [f for f in image_dir.iterdir() if f.is_file() and f.suffix.lower() in image_extensions]

    images_list = []

    for img_file in sorted(image_files):
        img_info = {
            'name': img_file.name,
            'size': img_file.stat().st_size,
            'path': str(img_file.relative_to(image_dir.parent))
        }

        # å¦‚æœéœ€è¦ä¸Šä¼ åˆ° MinIO
        if upload_to_minio:
            try:
                minio_client = get_minio_client()
                bucket_name = MINIO_CONFIG['bucket_name']
                minio_endpoint = MINIO_CONFIG['endpoint']

                # ç”Ÿæˆ UUID ä½œä¸ºæ–°æ–‡ä»¶å
                file_extension = img_file.suffix
                new_filename = f"{uuid.uuid4()}{file_extension}"
                object_name = f"images/{new_filename}"

                # ä¸Šä¼ åˆ° MinIO
                minio_client.fput_object(bucket_name=bucket_name, object_name=object_name, file_path=str(img_file))

                # ç”Ÿæˆè®¿é—® URL
                scheme = 'https' if MINIO_CONFIG['secure'] else 'http'
                img_info['url'] = f"{scheme}://{minio_endpoint}/{bucket_name}/{object_name}"

            except Exception as e:
                logger.error(f"Failed to upload image {img_file.name} to MinIO: {e}")
                img_info['url'] = None

        images_list.append(img_info)

    return {
        'count': len(images_list),
        'list': images_list,
        'uploaded_to_minio': upload_to_minio
    }


@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "service": "MinerU Tianshu",
        "version": "1.0.0",
        "description": "å¤©æ¢ - ä¼ä¸šçº§å¤šGPUæ–‡æ¡£è§£ææœåŠ¡",
        "docs": "/docs"
    }


@app.post("/api/v1/tasks/submit")
async def submit_task(
    file: UploadFile = File(..., description="æ–‡æ¡£æ–‡ä»¶: PDF/å›¾ç‰‡(MinerUè§£æ) æˆ– Office/HTML/æ–‡æœ¬ç­‰(MarkItDownè§£æ)"),
    backend: str = Form('pipeline', description="å¤„ç†åç«¯: pipeline/vlm-transformers/vlm-vllm-engine"),
    lang: str = Form('ch', description="è¯­è¨€: ch/en/korean/japanç­‰"),
    method: str = Form('auto', description="è§£ææ–¹æ³•: auto/txt/ocr"),
    formula_enable: bool = Form(True, description="æ˜¯å¦å¯ç”¨å…¬å¼è¯†åˆ«"),
    table_enable: bool = Form(True, description="æ˜¯å¦å¯ç”¨è¡¨æ ¼è¯†åˆ«"),
    priority: int = Form(0, description="ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§è¶Šä¼˜å…ˆ"),
):
    """
    æäº¤æ–‡æ¡£è§£æä»»åŠ¡
    
    ç«‹å³è¿”å› task_idï¼Œä»»åŠ¡åœ¨åå°å¼‚æ­¥å¤„ç†
    """
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix)
        
        # æµå¼å†™å…¥æ–‡ä»¶åˆ°ç£ç›˜ï¼Œé¿å…é«˜å†…å­˜ä½¿ç”¨
        while True:
            chunk = await file.read(1 << 23)  # 8MB chunks
            if not chunk:
                break
            temp_file.write(chunk)
        
        temp_file.close()
        
        # åˆ›å»ºä»»åŠ¡
        task_id = db.create_task(
            file_name=file.filename,
            file_path=temp_file.name,
            backend=backend,
            options={
                'lang': lang,
                'method': method,
                'formula_enable': formula_enable,
                'table_enable': table_enable,
            },
            priority=priority
        )
        
        logger.info(f"âœ… Task submitted: {task_id} - {file.filename} (priority: {priority})")
        
        return {
            'success': True,
            'task_id': task_id,
            'status': 'pending',
            'message': 'Task submitted successfully',
            'file_name': file.filename,
            'created_at': datetime.now().isoformat()
        }
    
    except Exception as e:
        logger.error(f"âŒ Failed to submit task: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/tasks/{task_id}/data")
async def get_task_data(
    task_id: str,
    include_fields: str = Query(
        "md,content_list,middle_json,model_output,images",
        description="éœ€è¦è¿”å›çš„å­—æ®µï¼Œé€—å·åˆ†éš”ï¼šmd,content_list,middle_json,model_output,images,layout_pdf,span_pdf,origin_pdf"
    ),
    upload_images: bool = Query(False, description="æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ°MinIOå¹¶è¿”å›URL"),
    include_metadata: bool = Query(True, description="æ˜¯å¦åŒ…å«æ–‡ä»¶å…ƒæ•°æ®")
):
    """
    æŒ‰éœ€è·å–ä»»åŠ¡çš„è§£ææ•°æ®

    æ”¯æŒçµæ´»è·å– MinerU è§£æåçš„æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š
    - Markdown å†…å®¹
    - Content List JSONï¼ˆç»“æ„åŒ–å†…å®¹åˆ—è¡¨ï¼‰
    - Middle JSONï¼ˆä¸­é—´å¤„ç†ç»“æœï¼‰
    - Model Output JSONï¼ˆæ¨¡å‹åŸå§‹è¾“å‡ºï¼‰
    - å›¾ç‰‡åˆ—è¡¨
    - å…¶ä»–è¾…åŠ©æ–‡ä»¶ï¼ˆlayout PDFã€span PDFã€origin PDFï¼‰

    é€šè¿‡ include_fields å‚æ•°æŒ‰éœ€é€‰æ‹©éœ€è¦è¿”å›çš„å­—æ®µ
    """
    # è·å–ä»»åŠ¡ä¿¡æ¯
    task = db.get_task(task_id)

    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    # æ„å»ºåŸºç¡€å“åº”
    response = {
        'success': True,
        'task_id': task_id,
        'status': task['status'],
        'file_name': task['file_name'],
        'backend': task['backend'],
        'created_at': task['created_at'],
        'completed_at': task['completed_at']
    }

    # å¦‚æœä»»åŠ¡æœªå®Œæˆï¼Œç›´æ¥è¿”å›çŠ¶æ€
    if task['status'] != 'completed':
        response['message'] = f"Task is in {task['status']} status, data not available yet"
        return response

    # æ£€æŸ¥ç»“æœè·¯å¾„
    if not task['result_path']:
        response['message'] = 'Task completed but result files have been cleaned up (older than retention period)'
        return response

    result_dir = Path(task['result_path'])
    if not result_dir.exists():
        response['message'] = 'Result directory does not exist'
        return response

    # è§£æéœ€è¦è¿”å›çš„å­—æ®µ
    fields = [f.strip() for f in include_fields.split(',')]

    # åˆå§‹åŒ– data å­—æ®µ
    response['data'] = {}  # type: ignore

    logger.info(f"ğŸ“¦ Getting complete data for task {task_id}, fields: {fields}")

    # æŸ¥æ‰¾æ–‡ä»¶ï¼ˆé€’å½’æœç´¢ï¼ŒMinerU è¾“å‡ºç»“æ„ï¼štask_id/filename/auto/*.mdï¼‰
    try:
        # 1. å¤„ç† Markdown æ–‡ä»¶
        if 'md' in fields:
            md_files = list(result_dir.rglob('*.md'))
            # æ’é™¤å¸¦ç‰¹æ®Šåç¼€çš„ md æ–‡ä»¶
            md_files = [f for f in md_files if not any(f.stem.endswith(suffix) for suffix in ['_layout', '_span', '_origin'])] 

            if md_files:
                md_file = md_files[0]
                logger.info(f"ğŸ“„ Reading markdown file: {md_file}")

                with open(md_file, 'r', encoding='utf-8') as f:
                    md_content = f.read()

                # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ä¸Šä¼ ï¼‰
                image_dir = md_file.parent / 'images'
                if upload_images and image_dir.exists():
                    md_content = process_markdown_images(md_content, image_dir, upload_images)

                response['data']['markdown'] = {
                    'content': md_content,
                    'file_name': md_file.name
                }

                if include_metadata:
                    metadata = get_file_metadata(md_file)
                    if metadata:
                        response['data']['markdown']['metadata'] = metadata

        # 2. å¤„ç† Content List JSON
        if 'content_list' in fields:
            content_list_files = list(result_dir.rglob('*_content_list.json'))
            if content_list_files:
                content_list_file = content_list_files[0]
                logger.info(f"ğŸ“„ Reading content list file: {content_list_file}")

                content_data = read_json_file(content_list_file)
                if content_data is not None:
                    response['data']['content_list'] = {
                        'content': content_data,
                        'file_name': content_list_file.name
                    }

                    if include_metadata:
                        metadata = get_file_metadata(content_list_file)
                        if metadata:
                            response['data']['content_list']['metadata'] = metadata

        # 3. å¤„ç† Middle JSON
        if 'middle_json' in fields:
            middle_json_files = list(result_dir.rglob('*_middle.json'))
            if middle_json_files:
                middle_json_file = middle_json_files[0]
                logger.info(f"ğŸ“„ Reading middle json file: {middle_json_file}")

                middle_data = read_json_file(middle_json_file)
                if middle_data is not None:
                    response['data']['middle_json'] = {
                        'content': middle_data,
                        'file_name': middle_json_file.name
                    }

                    if include_metadata:
                        metadata = get_file_metadata(middle_json_file)
                        if metadata:
                            response['data']['middle_json']['metadata'] = metadata

        # 4. å¤„ç† Model Output JSON
        if 'model_output' in fields:
            model_output_files = list(result_dir.rglob('*_model.json'))
            if model_output_files:
                model_output_file = model_output_files[0]
                logger.info(f"ğŸ“„ Reading model output file: {model_output_file}")

                model_data = read_json_file(model_output_file)
                if model_data is not None:
                    response['data']['model_output'] = {
                        'content': model_data,
                        'file_name': model_output_file.name
                    }

                    if include_metadata:
                        metadata = get_file_metadata(model_output_file)
                        if metadata:
                            response['data']['model_output']['metadata'] = metadata

        # 5. å¤„ç†å›¾ç‰‡
        if 'images' in fields:
            image_dirs = list(result_dir.rglob('images'))
            if image_dirs:
                image_dir = image_dirs[0]
                logger.info(f"ğŸ–¼ï¸  Getting images info from: {image_dir}")

                images_info = get_images_info(image_dir, upload_images)
                response['data']['images'] = images_info

        # 6. å¤„ç† Layout PDF
        if 'layout_pdf' in fields:
            layout_pdf_files = list(result_dir.rglob('*_layout.pdf'))
            if layout_pdf_files:
                layout_pdf_file = layout_pdf_files[0]
                response['data']['layout_pdf'] = {
                    'file_name': layout_pdf_file.name,
                    'path': str(layout_pdf_file.relative_to(result_dir))
                }

                if include_metadata:
                    metadata = get_file_metadata(layout_pdf_file)
                    if metadata:
                        response['data']['layout_pdf']['metadata'] = metadata

        # 7. å¤„ç† Span PDF
        if 'span_pdf' in fields:
            span_pdf_files = list(result_dir.rglob('*_span.pdf'))
            if span_pdf_files:
                span_pdf_file = span_pdf_files[0]
                response['data']['span_pdf'] = {
                    'file_name': span_pdf_file.name,
                    'path': str(span_pdf_file.relative_to(result_dir))
                }

                if include_metadata:
                    metadata = get_file_metadata(span_pdf_file)
                    if metadata:
                        response['data']['span_pdf']['metadata'] = metadata

        # 8. å¤„ç† Origin PDF
        if 'origin_pdf' in fields:
            origin_pdf_files = list(result_dir.rglob('*_origin.pdf'))
            if origin_pdf_files:
                origin_pdf_file = origin_pdf_files[0]
                response['data']['origin_pdf'] = {
                    'file_name': origin_pdf_file.name,
                    'path': str(origin_pdf_file.relative_to(result_dir))
                }

                if include_metadata:
                    metadata = get_file_metadata(origin_pdf_file)
                    if metadata:
                        response['data']['origin_pdf']['metadata'] = metadata

        logger.info(f"âœ… Complete data retrieved successfully for task {task_id}")

    except Exception as e:
        logger.error(f"âŒ Failed to get complete data for task {task_id}: {e}")
        logger.exception(e)
        raise HTTPException(status_code=500, detail=f"Internal server error: {e}")  

    return response


@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(
    task_id: str,
    upload_images: bool = Query(False, description="æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ°MinIOå¹¶æ›¿æ¢é“¾æ¥ï¼ˆä»…å½“ä»»åŠ¡å®Œæˆæ—¶æœ‰æ•ˆï¼‰"),
    add_page_numbers: bool = Query(False, description="æ˜¯å¦éœ€è¦åŠ ä¸Šé¡µç æ ‡è¯†ï¼ˆä»middle.jsonç”Ÿæˆå¸¦é¡µç çš„Markdownï¼‰")
):
    """
    æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€å’Œè¯¦æƒ…
    
    å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œä¼šè‡ªåŠ¨è¿”å›è§£æåçš„ Markdown å†…å®¹ï¼ˆdata å­—æ®µï¼‰
    å¯é€‰æ‹©æ˜¯å¦ä¸Šä¼ å›¾ç‰‡åˆ° MinIO å¹¶æ›¿æ¢ä¸º URL
    å¯é€‰æ‹©æ˜¯å¦ä» middle.json ç”Ÿæˆå¸¦é¡µç æ ‡è¯†çš„ Markdown
    """
    task = db.get_task(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    response = {
        'success': True,
        'task_id': task_id,
        'status': task['status'],
        'file_name': task['file_name'],
        'backend': task['backend'],
        'priority': task['priority'],
        'error_message': task['error_message'],
        'created_at': task['created_at'],
        'started_at': task['started_at'],
        'completed_at': task['completed_at'],
        'worker_id': task['worker_id'],
        'retry_count': task['retry_count']
    }
    logger.info(f"âœ… Task status: {task['status']} - (result_path: {task['result_path']})")
    
    # å¦‚æœä»»åŠ¡å·²å®Œæˆï¼Œå°è¯•è¿”å›è§£æå†…å®¹
    if task['status'] == 'completed':
        if not task['result_path']:
            # ç»“æœæ–‡ä»¶å·²è¢«æ¸…ç†
            response['data'] = None
            response['message'] = 'Task completed but result files have been cleaned up (older than retention period)'
            return response
        
        result_dir = Path(task['result_path'])
        logger.info(f"ğŸ“‚ Checking result directory: {result_dir}")
        
        if result_dir.exists():
            logger.info(f"âœ… Result directory exists")
            
            # æ ¹æ® add_page_numbers å‚æ•°é€‰æ‹©ä¸åŒçš„å¤„ç†é€»è¾‘
            if add_page_numbers:
                # æ–°é€»è¾‘ï¼šä» middle.json ç”Ÿæˆå¸¦é¡µç æ ‡è¯†çš„ Markdown
                logger.info(f"ğŸ“‘ Using page-numbered mode (from middle.json)")
                
                # é€’å½’æŸ¥æ‰¾ middle.json æ–‡ä»¶ï¼ˆæ ¼å¼ï¼šorigin.pdf.dVDzOLMD8_middle.jsonï¼‰
                middle_json_files = list(result_dir.rglob('*_middle.json'))
                logger.info(f"ğŸ“„ Found {len(middle_json_files)} middle.json files: {[f.relative_to(result_dir) for f in middle_json_files]}")
                
                if middle_json_files:
                    try:
                        # è¯»å– middle.json æ–‡ä»¶
                        middle_json_file = middle_json_files[0]
                        logger.info(f"ğŸ“– Reading middle.json file: {middle_json_file}")
                        
                        with open(middle_json_file, 'r', encoding='utf-8') as f:
                            middle_data = json.load(f)
                        
                        logger.info(f"âœ… Middle.json loaded, backend: {middle_data.get('_backend')}")
                        
                        # åˆ¤æ–­åç«¯ç±»å‹å¹¶é€‰æ‹©å¯¹åº”çš„ union_make å‡½æ•°
                        is_pipeline = middle_data.get('_backend') == 'pipeline'
                        if is_pipeline:
                            if pipeline_union_make is None:
                                raise ImportError("pipeline_union_make not available")
                            union_make = pipeline_union_make
                        else:
                            if vlm_union_make is None:
                                raise ImportError("vlm_union_make not available")
                            union_make = vlm_union_make
                        
                        # æŸ¥æ‰¾å›¾ç‰‡ç›®å½•ï¼ˆåœ¨ middle.json æ–‡ä»¶çš„åŒçº§ç›®å½•ä¸‹ï¼‰
                        image_dir = middle_json_file.parent / 'images'
                        img_bucket_path = str(image_dir) if image_dir.exists() else ''
                        
                        # ç”ŸæˆåŒ…å«åˆ†é¡µä¿¡æ¯çš„æ•´ä½“ Markdown
                        all_pages_md = []
                        pdf_info = middle_data.get('pdf_info', [])
                        
                        logger.info(f"ğŸ“‘ Processing {len(pdf_info)} pages")
                        
                        for page_info in pdf_info:
                            page_idx = page_info.get('page_idx', 0)
                            # æ˜¾ç¤ºæ—¶ä»1å¼€å§‹ï¼Œè€Œä¸æ˜¯ä»0å¼€å§‹
                            display_page_idx = page_idx + 1
                            
                            # union_make çš„ç¬¬ä¸€ä¸ªå‚æ•°åº”è¯¥æ˜¯ pdf_info åˆ—è¡¨ï¼Œè€Œä¸æ˜¯æ•´ä¸ªå­—å…¸
                            # ä¼ å…¥å•é¡µçš„ pdf_info åˆ—è¡¨
                            page_pdf_info = [page_info]
                            
                            # ç”Ÿæˆè¯¥é¡µçš„ Markdown
                            # union_make(pdf_info_dict: list, make_mode: str, img_buket_path: str = '')
                            page_md = union_make(page_pdf_info, 'mm_markdown', img_bucket_path)
                            
                            # æ·»åŠ é¡µé¢åˆ†éš”æ ‡è®°å’Œå†…å®¹ï¼ˆæ˜¾ç¤ºæ—¶ä»1å¼€å§‹ï¼‰
                            all_pages_md.append(f"\n\n--- Page {display_page_idx} ---\n\n")
                            all_pages_md.append(page_md)
                        
                        # åˆå¹¶ä¸ºæ•´ä½“ç»“æœ
                        final_md = ''.join(all_pages_md)
                        logger.info(f"âœ… Generated Markdown with {len(pdf_info)} pages, length: {len(final_md)} characters")
                        
                        # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if upload_images and image_dir.exists():
                            logger.info(f"ğŸ–¼ï¸  Processing images for task {task_id}, upload_images={upload_images}")
                            final_md = process_markdown_images(final_md, image_dir, upload_images)
                        
                        # æ·»åŠ  data å­—æ®µ
                        response['data'] = {
                            'middle_json_file': middle_json_file.name,
                            'content': final_md,
                            'pages': len(pdf_info),
                            'images_uploaded': upload_images,
                            'has_images': image_dir.exists() if not upload_images else None
                        }
                        logger.info(f"âœ… Response data field added successfully")
                        
                    except Exception as e:
                        logger.error(f"âŒ Failed to generate markdown from middle.json: {e}")
                        logger.exception(e)
                        # è¯»å–å¤±è´¥ä¸å½±å“çŠ¶æ€æŸ¥è¯¢ï¼Œåªæ˜¯ä¸è¿”å› data
                        response['data'] = None
                else:
                    logger.warning(f"âš ï¸  No middle.json files found in {result_dir}")
            else:
                # åŸæœ‰é€»è¾‘ï¼šç›´æ¥è¯»å–å·²ç”Ÿæˆçš„ Markdown æ–‡ä»¶
                logger.info(f"ğŸ“„ Using original mode (from markdown file)")
                
                # é€’å½’æŸ¥æ‰¾ Markdown æ–‡ä»¶ï¼ˆMinerU è¾“å‡ºç»“æ„ï¼štask_id/filename/auto/*.mdï¼‰
                md_files = list(result_dir.rglob('*.md'))
                logger.info(f"ğŸ“„ Found {len(md_files)} markdown files: {[f.relative_to(result_dir) for f in md_files]}")
                
                if md_files:
                    try:
                        # è¯»å– Markdown å†…å®¹
                        md_file = md_files[0]
                        logger.info(f"ğŸ“– Reading markdown file: {md_file}")
                        with open(md_file, 'r', encoding='utf-8') as f:
                            md_content = f.read()
                        
                        logger.info(f"âœ… Markdown content loaded, length: {len(md_content)} characters")
                        
                        # æŸ¥æ‰¾å›¾ç‰‡ç›®å½•ï¼ˆåœ¨ markdown æ–‡ä»¶çš„åŒçº§ç›®å½•ä¸‹ï¼‰
                        image_dir = md_file.parent / 'images'
                        
                        # å¤„ç†å›¾ç‰‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if upload_images and image_dir.exists():
                            logger.info(f"ğŸ–¼ï¸  Processing images for task {task_id}, upload_images={upload_images}")
                            md_content = process_markdown_images(md_content, image_dir, upload_images)
                        
                        # æ·»åŠ  data å­—æ®µ
                        response['data'] = {
                            'markdown_file': md_file.name,
                            'content': md_content,
                            'images_uploaded': upload_images,
                            'has_images': image_dir.exists() if not upload_images else None
                        }
                        logger.info(f"âœ… Response data field added successfully")
                        
                    except Exception as e:
                        logger.error(f"âŒ Failed to read markdown content: {e}")
                        logger.exception(e)
                        # è¯»å–å¤±è´¥ä¸å½±å“çŠ¶æ€æŸ¥è¯¢ï¼Œåªæ˜¯ä¸è¿”å› data
                        response['data'] = None
                else:
                    logger.warning(f"âš ï¸  No markdown files found in {result_dir}")
        else:
            logger.error(f"âŒ Result directory does not exist: {result_dir}")
    elif task['status'] == 'completed':
        logger.warning(f"âš ï¸  Task completed but result_path is empty")
    else:
        logger.info(f"â„¹ï¸  Task status is {task['status']}, skipping content loading")
    
    return response


@app.delete("/api/v1/tasks/{task_id}")
async def cancel_task(task_id: str):
    """
    å–æ¶ˆä»»åŠ¡ï¼ˆä»…é™ pending çŠ¶æ€ï¼‰
    """
    task = db.get_task(task_id)
    
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    if task['status'] == 'pending':
        db.update_task_status(task_id, 'cancelled')
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        file_path = Path(task['file_path'])
        if file_path.exists():
            file_path.unlink()
        
        logger.info(f"â¹ï¸  Task cancelled: {task_id}")
        return {
            'success': True,
            'message': 'Task cancelled successfully'
        }
    else:
        raise HTTPException(
            status_code=400, 
            detail=f"Cannot cancel task in {task['status']} status"
        )


@app.get("/api/v1/queue/stats")
async def get_queue_stats():
    """
    è·å–é˜Ÿåˆ—ç»Ÿè®¡ä¿¡æ¯
    """
    stats = db.get_queue_stats()
    
    return {
        'success': True,
        'stats': stats,
        'total': sum(stats.values()),
        'timestamp': datetime.now().isoformat()
    }


@app.get("/api/v1/queue/tasks")
async def list_tasks(
    status: Optional[str] = Query(None, description="ç­›é€‰çŠ¶æ€: pending/processing/completed/failed"),
    limit: int = Query(100, description="è¿”å›æ•°é‡é™åˆ¶", le=1000)
):
    """
    è·å–ä»»åŠ¡åˆ—è¡¨
    """
    if status:
        tasks = db.get_tasks_by_status(status, limit)
    else:
        # è¿”å›æ‰€æœ‰ä»»åŠ¡ï¼ˆéœ€è¦ä¿®æ”¹ TaskDB æ·»åŠ è¿™ä¸ªæ–¹æ³•ï¼‰
        with db.get_cursor() as cursor:
            cursor.execute('''
                SELECT * FROM tasks 
                ORDER BY created_at DESC 
                LIMIT ?
            ''', (limit,))
            tasks = [dict(row) for row in cursor.fetchall()]
    
    return {
        'success': True,
        'count': len(tasks),
        'tasks': tasks
    }


@app.post("/api/v1/admin/cleanup")
async def cleanup_old_tasks(days: int = Query(7, description="æ¸…ç†Nå¤©å‰çš„ä»»åŠ¡")):
    """
    æ¸…ç†æ—§ä»»åŠ¡è®°å½•ï¼ˆç®¡ç†æ¥å£ï¼‰
    """
    deleted_count = db.cleanup_old_tasks(days)
    
    logger.info(f"ğŸ§¹ Cleaned up {deleted_count} old tasks")
    
    return {
        'success': True,
        'deleted_count': deleted_count,
        'message': f'Cleaned up tasks older than {days} days'
    }


@app.post("/api/v1/admin/reset-stale")
async def reset_stale_tasks(timeout_minutes: int = Query(60, description="è¶…æ—¶æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰")):
    """
    é‡ç½®è¶…æ—¶çš„ processing ä»»åŠ¡ï¼ˆç®¡ç†æ¥å£ï¼‰
    """
    reset_count = db.reset_stale_tasks(timeout_minutes)
    
    logger.info(f"ğŸ”„ Reset {reset_count} stale tasks")
    
    return {
        'success': True,
        'reset_count': reset_count,
        'message': f'Reset tasks processing for more than {timeout_minutes} minutes'
    }


@app.get("/api/v1/health")
async def health_check():
    """
    å¥åº·æ£€æŸ¥æ¥å£
    """
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        stats = db.get_queue_stats()
        
        return {
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'database': 'connected',
            'queue_stats': stats
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return JSONResponse(
            status_code=503,
            content={
                'status': 'unhealthy',
                'error': str(e)
            }
        )


if __name__ == '__main__':
    # ä»ç¯å¢ƒå˜é‡è¯»å–ç«¯å£ï¼Œé»˜è®¤ä¸º8000
    api_port = int(os.getenv('API_PORT', '8000'))
    
    logger.info("ğŸš€ Starting MinerU Tianshu API Server...")
    logger.info(f"ğŸ“– API Documentation: http://localhost:{api_port}/docs")
    
    uvicorn.run(
        app, 
        host='0.0.0.0', 
        port=api_port,
        log_level='info'
    )

